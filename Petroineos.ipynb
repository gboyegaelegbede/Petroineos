{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ca3b6d-1172-424e-85bf-f04582055149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting et-xmlfile (from openpyxl)\n  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl.metadata\n  Using cached et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\nUsing cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\nUsing cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Check if this is not the initial run of this notebook i.e.; table named \"result\" exists\n",
    "count the number of columns in the table named \"result\n",
    "If the nos of columns is less than nos of columns in existing file at the website\n",
    "It means a new data (columns) has been added to the existing file at the website.\n",
    "Append the new data to the existing file.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "table_exists = spark.sql(\"SHOW TABLES LIKE 'result'\").count() > 0\n",
    "\n",
    "if table_exists:\n",
    "\n",
    "\n",
    "    # Read the new Excel file and load the \"Quarter\" sheet into a Pandas DataFrame\n",
    "    df_new = pd.read_excel(\"https://assets.publishing.service.gov.uk/media/66a76ff1ab418ab055592e8a/ET_3.1_JUL_24.xlsx\", sheet_name=\"Quarter\", header=4)\n",
    "\n",
    "    # Convert the Pandas DataFrame to a Spark DataFrame\n",
    "    df_new = spark.createDataFrame(df_new)\n",
    "\n",
    "    # Check for new data by comparing the nos of columns in new Spark DataFrame with the existing one\n",
    "\n",
    "    previous_columns = len(spark.table(\"Results\").columns)\n",
    "    new_columns = len(df_new.columns)\n",
    "\n",
    "    if (new_columns > previous_columns):\n",
    "\n",
    "        from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "        # Load the two tables as DataFrames\n",
    "        df_table2 = spark.table(\"Results\")\n",
    "        df_table1 = df_new\n",
    "\n",
    "        # Get the name of the last column in table1\n",
    "        last_column_name = df_table1.columns[-1]\n",
    "\n",
    "        # Select the last column from table1\n",
    "        last_column_df = df_table1.select(last_column_name)\n",
    "\n",
    "        # Add a monotonically increasing id to both DataFrames to ensure a one-to-one mapping\n",
    "        df_table2_with_id = df_table2.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "        last_column_df_with_id = last_column_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "        # Join the last column of table1 to table2 based on the row_id and then drop the row_id\n",
    "\n",
    "        df_table2_with_last_column = df_table2_with_id.join(last_column_df_with_id, df_table2_with_id[\"row_id\"] == last_column_df_with_id[\"row_id\"], \"left\").drop(\"row_id\")\n",
    "\n",
    "         # Write the Spark DataFrame with new column appended to a csv table\n",
    "         \n",
    "        df_table2_with_last_column.write.format(\"csv\").mode(\"overwrite\").saveAsTable(\"Results\")\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    # It is an initial load, simply write the data to the csv table\n",
    "    \n",
    "    # Read the Excel file and load the \"Quarter\" sheet into a Pandas DataFrame\n",
    "    df = pd.read_excel(\"https://assets.publishing.service.gov.uk/media/66a76ff1ab418ab055592e8a/ET_3.1_JUL_24.xlsx\", sheet_name=\"Quarter\", header=4)\n",
    "\n",
    "    # Convert the Pandas DataFrame to a Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Write the Spark DataFrame to a csv table\n",
    "    spark_df.write.format(\"csv\").mode(\"overwrite\").saveAsTable(\"Results\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Petroineos",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
